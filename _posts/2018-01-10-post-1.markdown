---
layout: archive
title: "Awards"
permalink: /awards/
author_profile: true
redirect_from:
  - /resume
---

{% include base_path %}


* Ranked #5 in Kaggle Half Earth Challenge *Herbarium* 2022
* Contributed to Arctic Code Vault, GitHub 2020
  * GitHub has partnered with the Long Now Foundation, the Internet Archive, the Software Heritage Foundation, Arctic World Archive, Microsoft Research, the Bodleian Library, and Stanford Libraries to ensure the long-term preservation of the world's open source software. We will protect this priceless knowledge by storing multiple copies, on an ongoing basis, across various data formats and locations, including a very-long-term archive designed to last at least 1,000 years.
* PhD scholarship from ANR under ANITI project 2019-2022
  * The French government, in consultation with an international jury of experts, has selected ANITI to be one of four, highly visible interdisciplinary institutes spearheading AI research, education, and economic development in France. ANITI has targeted as strategic areas mobility and transportation, and robotics/cobotics for the industry of the future.
* Charpak Masters Scholarship from French Government, 2017



* **AR and TAR** : AR (Activation Regularization) and TAR (Temporal Activation Regularization) are modifications of $$L_2$$ regularization, wherein the standard technique is applied to dropped *output activations* and dropped *change in output activations* respectively. Mathematically, the additional terms in the cost function $$J$$ are (here $$\alpha$$ and $$\beta$$ are scaling constants and $$\textbf{D}$$ is the dropout mask) :

$$
J_{AR}=\alpha L_2\left(\textbf{D}_l^t\odot h_l^t\right)\\
J_{TAR}=\beta L_2\left(\textbf{D}_l^t\odot\left(h_l^t - h_l^{t-1}\right)\right)
$$

* **Weight tying** : In this method, the parameters for word embeddings and the final output layer are shared.
* **Variable backpropagation steps** : A random number of BPTT steps are taken instead of a fixed number, whose mean is very close to the original fixed value ($$s$$). The BPTT step-size ($$x$$) is drawn from the following distribution (here $$\mathcal{N}$$ is the Gaussian distribution, $$p$$ is a number close to 0.95 and $$\sigma^2$$ is the desired variance) :

$$
x \sim p\cdot \mathcal{N}\left(s,\sigma^2\right) + (1-p)\cdot \mathcal{N}\left(\frac{s}{2},\sigma^2\right)
$$

* **Independent sizes of word embeddings and hidden layer** : The sizes of the hidden layer and word embeddings are kept independent of each other.

The paper also introduces a new optimization algorithm, namely **Non-monotonically Triggered Averaged Stochastic Gradient Descent** or NT-ASGD, which can be programmatically described as follows :
{% highlight python %}
def NT_ASGD(f, t, w0, n, L, lr):
    """
    Input parameters :
    f  - objective function
    t  - stopping criterion
    w0 - initial parameters
    n  - non-monotonicity interval
    L  - number of epochs after which finetuning is done
    lr - learning rate

    Returns :
    parameter(s) that minimize `f`
    """
    k = 0; T = 0; t = 0; params = []; logs = []
    w = w0
    while t(w):
        # `func_grad` computes gradient of `f` at `w`
        w = w - lr * func_grad(f, w)
        params.append(w)
        k += 1
        if k%L == 0:
            # Compute model's perplexity for current parameters
            v = perplexity(w)
            if t > n and v > min(logs[t-n:t+1]):
                T = k
            logs.append(v)
            t += 1
    # Return the average of best `k-T+1` parameters
    return sum(params[-(k-T+1):])/(k-T+1)     
{% endhighlight %}
They also combined their **AWD-LSTM** (ASGD Weight Dropped LSTM) with a neural cache model to obtain further reduction in perplexities. A *neural cache model* stores previous states in memory, and predicts the output obtained by a *convex combination* of the output using stored states and the AWD-LSTM.

### Network description
*Merity et al*'s model used a 3-layer weight dropped LSTM with dropout probability `0.5` for **PTB corpus** and `0.65` for **WikiText-2**, combined with several of the above regularization techniques. The different hyperparameters (as referred to in the discussion above) are as follows : hidden layer size ($$H$$) = `1150`, embedding size ($$D$$) = `400`, number of epochs = `750`, $$L$$ = `1`, $$n$$ = `5`, learning rate = `30`, Gradients clipped at `0.25`, $$p$$ = `0.95`, $$s$$ = `70`, $$\sigma^2$$ = `5`, $$\alpha$$ = `2`, $$\beta$$ = `1`, dropout probabilities for input, hidden outputs, final output and embeddings as `0.4`, `0.3`, `0.4` and `0.1` respectively.

Word embedding weights were initialized from $$\mathcal{U}\left[-0.1,0.1\right]$$ and all other hidden weights from $$\mathcal{U}\left[-\frac{1}{\sqrt{1150}},\frac{1}{\sqrt{1150}}\right]$$. Mini-batch size of `40` was used for PTB and `80` for WT-2.

### Result highlights
* 3-layer AWD-LSTM with weight tying attained 57.3 PPL on PTB
* 3-layer AWD-LSTM with weight tying and a continuous cache pointer attained 52.8 PPL on PTB
